{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet -U watermark nb_black ipython-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet -U tabulate torch ipywidgets numpy seaborn spacy textacy nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import watermark\n",
    "%load_ext watermark\n",
    "%load_ext autotime\n",
    "%load_ext nb_black\n",
    "%gui asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "import re\n",
    "from spacy import displacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.matcher import Matcher\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark -i -n -v -m -iv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below just once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://realpython.com/natural-language-processing-spacy-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "custom_nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction_doc = nlp(\n",
    "    \"This tutorial is about Natural Language Processing in spaCy.\"\n",
    ")\n",
    "type(introduction_doc)\n",
    "\n",
    "\n",
    "[token.text for token in introduction_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    ")\n",
    "about_doc = nlp(about_text)\n",
    "sentences = list(about_doc.sents)\n",
    "len(sentences)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(f\"{sentence[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ellipsis_text = (\n",
    "    \"Gus, can you, ... never mind, I forgot\"\n",
    "    \" what I was saying. So, do you think\"\n",
    "    \" we should ...\"\n",
    ")\n",
    "\n",
    "@Language.component(\"set_custom_boundaries\")\n",
    "def set_custom_boundaries(doc):\n",
    "    \"\"\"Add support to use `...` as a delimiter for sentence detection\"\"\"\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == \"...\":\n",
    "            doc[token.i + 1].is_sent_start = True\n",
    "    return doc\n",
    "custom_nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")\n",
    "custom_ellipsis_doc = custom_nlp(ellipsis_text)\n",
    "custom_ellipsis_sentences = list(custom_ellipsis_doc.sents)\n",
    "for sentence in custom_ellipsis_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    ")\n",
    "about_doc = nlp(about_text)\n",
    "\n",
    "for token in about_doc:\n",
    "    print (token, token.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'{\"Text with Whitespace\":22}'\n",
    "    f'{\"Is Alphanumeric?\":15}'\n",
    "    f'{\"Is Punctuation?\":18}'\n",
    "    f'{\"Is Stop Word?\"}'\n",
    ")\n",
    "for token in about_doc:\n",
    "    print(\n",
    "        f\"{str(token.text_with_ws):22}\"\n",
    "        f\"{str(token.is_alpha):15}\"\n",
    "        f\"{str(token.is_punct):18}\"\n",
    "        f\"{str(token.is_stop)}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_nlp = spacy.load(\"en_core_web_sm\")\n",
    "prefix_re = spacy.util.compile_prefix_regex(\n",
    "    custom_nlp.Defaults.prefixes\n",
    ")\n",
    "suffix_re = spacy.util.compile_suffix_regex(\n",
    "    custom_nlp.Defaults.suffixes\n",
    ")\n",
    "\n",
    "custom_infixes = [r\"@\"]\n",
    "\n",
    "infix_re = spacy.util.compile_infix_regex(\n",
    "    list(custom_nlp.Defaults.infixes) + custom_infixes\n",
    ")\n",
    "\n",
    "custom_nlp.tokenizer = Tokenizer(\n",
    "    nlp.vocab,\n",
    "    prefix_search=prefix_re.search,\n",
    "    suffix_search=suffix_re.search,\n",
    "    infix_finditer=infix_re.finditer,\n",
    "    token_match=None,\n",
    ")\n",
    "\n",
    "custom_about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London@based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    ")\n",
    "\n",
    "custom_tokenizer_about_doc = custom_nlp(custom_about_text)\n",
    "print([token.text for token in custom_tokenizer_about_doc[8:15]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "len(spacy_stopwords)\n",
    "\n",
    "for stop_word in list(spacy_stopwords)[:10]:\n",
    "    print(stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    ")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "about_doc = nlp(custom_about_text)\n",
    "print([token for token in about_doc if not token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_help_text = (\n",
    "    \"Gus is helping organize a developer\"\n",
    "    \" conference on Applications of Natural Language\"\n",
    "    \" Processing. He keeps organizing local Python meetups\"\n",
    "    \" and several internal talks at his workplace.\"\n",
    ")\n",
    "conference_help_doc = nlp(conference_help_text)\n",
    "for token in conference_help_doc:\n",
    "    if str(token) != str(token.lemma_):\n",
    "        print(f\"{str(token):>20} : {str(token.lemma_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech company. He is\"\n",
    "    \" interested in learning Natural Language Processing.\"\n",
    "    \" There is a developer conference happening on 21 July\"\n",
    "    ' 2019 in London. It is titled \"Applications of Natural'\n",
    "    ' Language Processing\". There is a helpline number'\n",
    "    \" available at +44-1234567891. Gus is helping organize it.\"\n",
    "    \" He keeps organizing local Python meetups and several\"\n",
    "    \" internal talks at his workplace. Gus is also presenting\"\n",
    "    ' a talk. The talk will introduce the reader about \"Use'\n",
    "    ' cases of Natural Language Processing in Fintech\".'\n",
    "    \" Apart from his work, he is very passionate about music.\"\n",
    "    \" Gus is learning to play the Piano. He has enrolled\"\n",
    "    \" himself in the weekend batch of Great Piano Academy.\"\n",
    "    \" Great Piano Academy is situated in Mayfair or the City\"\n",
    "    \" of London and has world-class piano instructors.\"\n",
    ")\n",
    "complete_doc = nlp(complete_text)\n",
    "\n",
    "words = [\n",
    "    token.text\n",
    "    for token in complete_doc\n",
    "    if not token.is_stop and not token.is_punct\n",
    "]\n",
    "\n",
    "print(Counter(words).most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(\n",
    "    [token.text for token in complete_doc if not token.is_punct]\n",
    ").most_common(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    ")\n",
    "about_doc = nlp(about_text)\n",
    "for token in about_doc:\n",
    "    print(\n",
    "        f\"\"\"\n",
    "TOKEN: {str(token)}\n",
    "=====\n",
    "TAG: {str(token.tag_):10} POS: {token.pos_}\n",
    "EXPLANATION: {spacy.explain(token.tag_)}\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = []\n",
    "adjectives = []\n",
    "for token in about_doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == \"ADJ\":\n",
    "        adjectives.append(token)\n",
    "\n",
    "print(nouns)\n",
    "print(adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "about_interest_text = (\n",
    "    \"He is interested in learning Natural Language Processing.\"\n",
    ")\n",
    "about_interest_doc = nlp(about_interest_text)\n",
    "displacy.render(about_interest_doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "complete_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech company. He is\"\n",
    "    \" interested in learning Natural Language Processing.\"\n",
    "    \" There is a developer conference happening on 21 July\"\n",
    "    ' 2019 in London. It is titled \"Applications of Natural'\n",
    "    ' Language Processing\". There is a helpline number'\n",
    "    \" available at +44-1234567891. Gus is helping organize it.\"\n",
    "    \" He keeps organizing local Python meetups and several\"\n",
    "    \" internal talks at his workplace. Gus is also presenting\"\n",
    "    ' a talk. The talk will introduce the reader about \"Use'\n",
    "    ' cases of Natural Language Processing in Fintech\".'\n",
    "    \" Apart from his work, he is very passionate about music.\"\n",
    "    \" Gus is learning to play the Piano. He has enrolled\"\n",
    "    \" himself in the weekend batch of Great Piano Academy.\"\n",
    "    \" Great Piano Academy is situated in Mayfair or the City\"\n",
    "    \" of London and has world-class piano instructors.\"\n",
    ")\n",
    "complete_doc = nlp(complete_text)\n",
    "def is_token_allowed(token):\n",
    "    return bool(\n",
    "        token\n",
    "        and str(token).strip()\n",
    "        and not token.is_stop\n",
    "        and not token.is_punct\n",
    "    )\n",
    "\n",
    "def preprocess_token(token):\n",
    "    return token.lemma_.strip().lower()\n",
    "\n",
    "complete_filtered_tokens = [\n",
    "    preprocess_token(token)\n",
    "    for token in complete_doc\n",
    "    if is_token_allowed(token)\n",
    "]\n",
    "\n",
    "complete_filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    ")\n",
    "about_doc = nlp(about_text)\n",
    "\n",
    "# from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def extract_full_name(nlp_doc):\n",
    "    pattern = [{\"POS\": \"PROPN\"}, {\"POS\": \"PROPN\"}]\n",
    "    matcher.add(\"FULL_NAME\", [pattern])\n",
    "    matches = matcher(nlp_doc)\n",
    "    for _, start, end in matches:\n",
    "        span = nlp_doc[start:end]\n",
    "        yield span.text\n",
    "\n",
    "next(extract_full_name(about_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_org_text = (\"There is a developer conference\"\n",
    "    \" happening on 21 July 2019 in London. It is titled\"\n",
    "    ' \"Applications of Natural Language Processing\".'\n",
    "    \" There is a helpline number available\"\n",
    "    \" at (123) 456-7891\")\n",
    "\n",
    "\n",
    "def extract_phone_number(nlp_doc):\n",
    "    pattern = [\n",
    "        {\"ORTH\": \"(\"},\n",
    "        {\"SHAPE\": \"ddd\"},\n",
    "        {\"ORTH\": \")\"},\n",
    "        {\"SHAPE\": \"ddd\"},\n",
    "        {\"ORTH\": \"-\", \"OP\": \"?\"},\n",
    "        {\"SHAPE\": \"dddd\"},\n",
    "    ]\n",
    "    matcher.add(\"PHONE_NUMBER\", [pattern])\n",
    "    matches = matcher(nlp_doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_doc[start:end]\n",
    "        return span.text\n",
    "\n",
    "\n",
    "conference_org_doc = nlp(conference_org_text)\n",
    "extract_phone_number(conference_org_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piano_text = \"Gus is learning piano\"\n",
    "piano_doc = nlp(piano_text)\n",
    "for token in piano_doc:\n",
    "    print(\n",
    "        f\"\"\"\n",
    "TOKEN: {token.text}\n",
    "=====\n",
    "{token.tag_ = }\n",
    "{token.head.text = }\n",
    "{token.dep_ = }\"\"\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(piano_doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_line_about_text = (\n",
    "    \"Gus Proto is a Python developer\"\n",
    "    \" currently working for a London-based Fintech company\"\n",
    ")\n",
    "one_line_about_doc = nlp(one_line_about_text)\n",
    "\n",
    "# Extract children of `developer`\n",
    "print([token.text for token in one_line_about_doc[5].children])\n",
    "\n",
    "\n",
    "# Extract previous neighboring node of `developer`\n",
    "print (one_line_about_doc[5].nbor(-1))\n",
    "\n",
    "\n",
    "# Extract next neighboring node of `developer`\n",
    "print (one_line_about_doc[5].nbor())\n",
    "\n",
    "\n",
    "# Extract all tokens on the left of `developer`\n",
    "print([token.text for token in one_line_about_doc[5].lefts])\n",
    "\n",
    "\n",
    "# Extract tokens on the right of `developer`\n",
    "print([token.text for token in one_line_about_doc[5].rights])\n",
    "\n",
    "\n",
    "# Print subtree of `developer`\n",
    "print (list(one_line_about_doc[5].subtree))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_text = (\n",
    "    \"There is a developer conference happening on 21 July 2019 in London.\"\n",
    ")\n",
    "conference_doc = nlp(conference_text)\n",
    "\n",
    "# Extract Noun Phrases\n",
    "for chunk in conference_doc.noun_chunks:\n",
    "    print (chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import textacy\n",
    "\n",
    "about_talk_text = (\n",
    "    \"The talk will introduce reader about use\"\n",
    "    \" cases of Natural Language Processing in\"\n",
    "    \" Fintech, making use of\"\n",
    "    \" interesting examples along the way.\"\n",
    ")\n",
    "\n",
    "patterns = [{\"POS\": \"AUX\"}, {\"POS\": \"VERB\"}]\n",
    "about_talk_doc = textacy.make_spacy_doc(\n",
    "    about_talk_text, lang=\"en_core_web_sm\"\n",
    ")\n",
    "verb_phrases = textacy.extract.token_matches(\n",
    "    about_talk_doc, patterns=patterns\n",
    ")\n",
    "\n",
    "# Print all verb phrases\n",
    "for chunk in verb_phrases:\n",
    "    print(chunk.text)\n",
    "\n",
    "# Extract noun phrase to explain what nouns are involved\n",
    "for chunk in about_talk_doc.noun_chunks:\n",
    "    print (chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piano_class_text = (\n",
    "    \"Great Piano Academy is situated\"\n",
    "    \" in Mayfair or the City of London and has\"\n",
    "    \" world-class piano instructors.\"\n",
    ")\n",
    "piano_class_doc = nlp(piano_class_text)\n",
    "\n",
    "for ent in piano_class_doc.ents:\n",
    "    print(\n",
    "        f\"\"\"\n",
    "{ent.text = }\n",
    "{ent.start_char = }\n",
    "{ent.end_char = }\n",
    "{ent.label_ = }\n",
    "spacy.explain('{ent.label_}') = {spacy.explain(ent.label_)}\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(piano_class_doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_text = (\n",
    "    \"Out of 5 people surveyed, James Robert,\"\n",
    "    \" Julie Fuller and Benjamin Brooks like\"\n",
    "    \" apples. Kelly Cox and Matthew Evans\"\n",
    "    \" like oranges.\"\n",
    ")\n",
    "\n",
    "\n",
    "def replace_person_names(token):\n",
    "    if token.ent_iob != 0 and token.ent_type_ == \"PERSON\":\n",
    "        return \"[REDACTED] \"\n",
    "    return token.text_with_ws\n",
    "\n",
    "\n",
    "def redact_names(nlp_doc):\n",
    "    with nlp_doc.retokenize() as retokenizer:\n",
    "        for ent in nlp_doc.ents:\n",
    "            retokenizer.merge(ent)\n",
    "    tokens = map(replace_person_names, nlp_doc)\n",
    "    return \"\".join(tokens)\n",
    "\n",
    "\n",
    "survey_doc = nlp(survey_text)\n",
    "print(redact_names(survey_doc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dcd39a1a4fd5d1ce925d3608d51b40b2a0714e33960bbfca83a9d0704a93f2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
